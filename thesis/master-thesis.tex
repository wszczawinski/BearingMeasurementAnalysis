\documentclass[nostrict]{PG-schema}

\usepackage[unicode=true]{hyperref}
\newcommand\PDFtitle{Analysis of the plain bearings measurements results using data science methods.}
\newcommand\PDFauthors{Wojciech Szczawiñski}
\hypersetup{
  pdftitle={\PDFtitle},
  pdfauthor={\PDFauthors},
  pdfsubject={Master Thesis},
  pdfkeywords={master thesis, mechanical engineering, data science, plain bearings, analyses}
 }

\usepackage{multicol} % for two columns set

%---- only for this file purposes
\makeatletter
\renewcommand{\verbatim@font}{\ttfamily\small}
\makeatother
%-----

\begin{document}

\tableofcontents    % table of content
\listoffigures      % list of figures


\chapter{Defining the purpose of the thesis}

The main objective of this paper is an attempt to use the tools provided by Data Science to analyse data from measurements made for research work in mechanical engineering.

In some applications, a conventional data analysis methods for mechanical engineering are beginning to be insufficient and a limitation to obtaining the best possible conclusions from measurements. Here it would be useful to look around at what data analysis capabilities have been developed in recent years and try to implement them for mechanical engineering. The leading branch of science focusing on data analysis in recent years is actually Data Science and it seems that using the tools that have been developed there can bring a lot of benefits to processing measurements data. 

Data science methods are not limited by the existing software - one can freely shape the interface and use any statistical tools. Data science tools based on python and R libraries are optimised for processing large databases, use of machine learning methods is increasingly used to make predictions based on previous experiences.

Thanks to the fact that most of the libraries around Python and R are open source and built by the community, they very often respond directly to the needs of users and are constantly extended and new ones appear. More information about programming languages and available tools will be presented in Chapter 4.

\section{Preliminary definition of Data Science}

To begin with, it seems appropriate to give an overview of what Data Science is, the origin of this branch of science and what branches of science it is related to. The following diagram, presented below on figure \ref{data-science-graph}, can be useful for this purpose; it shows the overlap of the branches of science that data science combines.

\begin{figure}[htbp]
  \begin{center}
    \resizebox{0.7\textwidth}{!}{
      \includegraphics{images/data-science-graph.png}
    }
    \caption[Data Science graph.]{Data Science graph. \cite{dsgraph} }
    \label{data-science-graph}
  \end{center}
\end{figure}

One can then recall a short definition, the unfolding of which, combined with the diagram above, should give a fairly good intuition about the complexity of this domain.

\begin{quote}
\emph{"Data science combines the scientific method, math and statistics, specialised programming, advanced analytics, AI, and even storytelling to uncover and explain the business insights buried in data."} \cite{dsdefinition} 
\end{quote}

First of all Data Science is the domain of study that deals with vast volumes of data which are collected from different sources. Here we are dealing with the lowest circle in the diagram \ref{data-science-graph}, the data can come from a business that wants to analyse its actions (for example, it can be traffic data from a store's website, or GPS data from trucks of a transport company) or from specialised research from which we want to draw the greatest insight. The data used for analysis can be from multiple sources and present in various formats.

Data Science uses modern tools and techniques that come from both circles on the top of graph \ref{data-science-graph}. Techniques derived from mathematics and statistics provide long-established capabilities for computing, analysing, and processing data. Computer science, on the other hand, provides the ability to perform these calculations on large data sets and curate machine learning algorithms to build predictive models. 

The combination of all this gives  the ability to find unseen patterns, extract meaningful information and make business decisions that would not be possible with conventional solutions.

The above short definition shows that it is a field of science which takes a multidisciplinary approach and is aimed at the most accessible presentation of data in a business environment where it is often necessary to show the results of some actions to people who are not involved in a given field. Using these methods focuses on getting the most utility out of data, which in today's world is a large and often growing set.  

This short introduction will be developed in one of the following chapters (Chapter 4) where aspects such as: lifecycles, tools, use cases and more will be presented in depth.

\section{Acquisition of measurement data for analysis}

Thanks to the possibility of cooperation with the department of mechanical engineering, it was possible to obtain data resulting from the work on plain bearings that they conduct. Initial discussions revealed a need to expand the range of tools for analysing the data obtained from this research for reasons discussed in more depth in Chapter 2 of this thesis. Thanks to this possibility, it was possible to obtain real data from measurements and there was no need to create measurements or even simulate what measurements would be just to complete this paper.

Another advantage will be the possibility of comparing the results of analyses based on data science methods to analyses conducted by a unit that deals with these bearings on a daily basis, and possibly supporting their work with tools that can be selected in further stages of this work. In the case of the team that deals with these measurements, they have already been in contact with data scientists from outside the university in the form of orders to create software for processing data from specific measurements. From this point of view, it seems all the more reasonable to combine efforts to explore the topic of data analysis using data science methods.

\section{Methodology for the analysis}

The preliminary idea of the work presented in this paper is to analyse the results of measurements using selected data science methods and to compare these results with the results of analysis performed using conventional methods of mechanical measurement analysis that have been used so far by the team who perform the measurements.

\chapter{Identifying problems that arise with the collection of an increasing amount of measurement data}

Improvements in the quality of measuring instruments and increase in their precision allow us to perform measurements with increasing accuracy and resolution. Thus, the amount of data that we obtain from measurements is also growing and it seems that this trend will continue in the coming years. Hence, there is a need to increase the possibility of processing these data, preferably in a complete free way and giving the flexibility to choose tools for their analysis from the wildest possible spectrum. In such a way as to be able to adopt to the incoming new data in case of a desire to extend measurements or change the measuring equipment and thus add new output results or possibility of adopting a different data structure.

Data processing methods, which have been used so far in the process of measurement in mechanical engineering starts to limit possibilities of extracting the greatest possible amount of informations from the measurements.

\section{Performance problems that arise with large data sets}

For measurements with the latest high-resolution sensors, a single measurement can consist of several thousand measurement points or more. Spreadsheet-type graphical programs start to be difficult to use in such cases, and problems may occur from the very moment of viewing the data. Further, there are limited possibilities of data cleaning as well as limitations with presenting data in the form of graphs, especially when it comes to graphs presenting three-dimensional measurements.

\section{Reusable analyses for similar datasets}

If we consider the case in which we want to measure a large number of similar elements - in this case sleeves that are sliding bearings, there arises a desire to have a tool that will enable automatic start of analysis on successive sets of data from measurements. 

With conventional programs, this requires duplicating a spreadsheet and pasting more data there. In case of a script written for a given type of measurement it is enough to run it on another file and almost immediately we get desired graphs or any other form of results.

\section{Functional limitations }



\chapter{Searching for available methods of data analysis already used in mechanical engineering.}

When looking for a reference point, it is necessary to trace the software used so far for analysing the measurement data. In order to narrow down the spectrum, the focus has been on the software used to date to process data from plain bearings measurements which will later be analysed using data science methods. 

\section{Excel (Microsoft)}

The first thing that comes to mind when analysing data is, of course, Excel. The most well-known, easy-to-use and available software that is used for all kinds of analysis, processing and viewing data. The interface of the program is shown in Figure \ref{excel-gui}.

\begin{figure}[htbp]
  \begin{center}
    \resizebox{0.7\textwidth}{!}{
      \includegraphics{images/excel.png}
    }
    \caption[Excel GUI]{GUI of Excel software. \cite{linkexcel} }
    \label{excel-gui}
  \end{center}
\end{figure}

\begin{quote}
\emph{"Microsoft Excel provides a grid interface to organise nearly any type of information.  The power of Excel lies in it is flexibility to define the layout and structure of the information you want to manage.  Basic tasks require no special training, and Excel allows you to work with text, numbers, and date information in a relatively open and unstructured way.  Nearly 30 years after it is initial introduction, Excel remains the worlds leading spreadsheet software." } \cite{linkexceldefinition}
\end{quote}

\section{Origin (OriginLab)}

Origin is GUI based software aimed at scientists and therefore has a lot of capabilities often needed for measurement analysis. The software has a lot of available options, add-ons and possibilities to work with other programs. Graphic user interface of Origin software is shown on Figure \ref{origin-gui}.

\begin{figure}[htbp]
  \begin{center}
    \resizebox{0.7\textwidth}{!}{
      \includegraphics{images/origin.png}
    }
    \caption[Origin GUI]{Origin GUI. \cite{linkorigin} }
    \label{origin-gui}
  \end{center}
\end{figure}

\begin{quote}
\emph{" Origin is the data analysis and graphing software of choice for over half a million scientists and engineers in commercial industries, academia, and government laboratories worldwide. Origin offers an easy-to-use interface for beginners, combined with the ability to perform advanced customisation as you become more familiar with the application. Origin graphs and analysis results can automatically update on data or parameter change, allowing you to create templates for repetitive tasks or to perform batch operations from the user interface, without the need for programming. Extend the capabilities in Origin by installing free Apps available from our website."} \cite{linkorigin} 
\end{quote}

\section{LabVIEW (NI)}

LabVIEW is used to create a measurement strategy, build a test bench, and quickly view real-time data. Its capabilities are quite broad and it is a good test environment because of the ease of making changes and access to simple visualisation methods immediately after measurements. An example of the LabVIEW graphical interface can be seen in Figure \ref{labview-gui}

\begin{figure}[htbp]
  \begin{center}
    \resizebox{0.7\textwidth}{!}{
      \includegraphics{images/labview.png}
    }
    \caption[LabVIEW GUI]{LabVIEW GUI. \cite{linklabview} }
    \label{labview-gui}
  \end{center}
\end{figure}

\begin{quote}
\emph{"LabVIEW is systems engineering software for applications that require test, measurement, and control with rapid access to hardware and data insights. LabVIEW offers a graphical programming approach that helps you visualise every aspect of your application, including hardware configuration, measurement data, and debugging. This visualisation makes it simple to integrate measurement hardware from any vendor, represent complex logic on the diagram, develop data analysis algorithms, and design custom engineering user interfaces."} \cite{linklabview}  
\end{quote}

\section{Solartron Metrology}

In this particular case, one of the software that will be used in this research work will be a detector program from Solartron Metrology. Due to the measuring devices used, the program is needed to process the output signals from the measuring devices. The signal that arrives at the computer is then processed by the software and prepared for further analysis in the form of files such as .cvs. There is a simple interface here both to control the sampling method and to adjust the format of the output data. In the following discussion the role of the software from Solartron will be omitted because the focus is on the analysis of the data itself and not on the way of conducting the measurements.

\chapter{Presentation of the general outline of data science methods and the possibility of using them to analyse measurements.}



\section{Data science}



\section{Lifecycle of data science project}

When it comes to how a project is run in data science, there is a term called project lifecycle. This may differ in the way it is described as well as in the specific points depending on the source where you look for information about the project lifecycle, but in principle the main steps will be similar:

\subsection{Understanding problem}

Even before first insight into the data it is necessary to define and understand the problem which is a subject of analysis. Deep understanding of the problem principles is the most important in early stage because it can protect from loosing time for development of model which won't meet assumptions. In data science most of the projects is based on problems brought in from business or scientific demand and hence it is very important that the communication between those aggregating data and those doing strictly data science analysis is clear. Data analysis itself should not be in isolation from those doing the measuring, and there needs to be a constant exchange of insights and guidance on the desired output. 

\subsection{Data collection}

 Once you understand the problem, it is important to collect the right data sets. Data is the core of the whole problem and it is necessary to collect it in an appropriate way to be able to create a project based on it. In the case of this particular paper, the data is collected by researchers working in the field of plain bearings on a daily basis and on specially designed for these purposes test stands.

\subsection{Data cleaning and processing}

It is not enough to just collect data, because most of the time such raw data will not be suitable to create a model. Once collected, the data needs to be properly processed and cleaned before it can be used for the rest of the work. It is necessary to solve flaws and inconsistencies as well as deal with null values and outliers. 

\subsection{Exploratory data analysis}

At this stage of the project there is already data as well as it has been cleaned and pre-processed and organised. One can take a deeper look at this data to get familiar with the properties and features. There is also an occasion to build a so-called intuition about the data set and begin to think about a strategy for creating the model. After a closer look at the data, there is an opportunity to raise a deeper discussion with experts in the field, try to find the first patterns and visualisations.

\subsection{Model building and evaluation}

Although the previous phases of the project take a lot of time and are important, it is only during the building of the model that the most important things happen. The first thing is to properly divide the database into train and test set. The test set is used to evaluate the model and check whether it works correctly only on the dataset that was used to build it. This is especially concern in case of predictive models based on machine learning where in case of too small train data set or too many learning cycles performed on this set model can lose flexibility and be useless after loading new data set. 

Various metrics provided by the statistical domain can be used for evaluation. The evaluation methods will depend on the specific problem, and after the first feedback, adjustments to the model can be made. 

\subsection{Communicating model results}



\subsection{Model deployment and maintenance}



\section{Programming language (Python)}

In this particular paper, the programming language used for data analysis will be Python, which main advantages are versatility, simple syntax, multitude of available libraries, large user base, ever-growing popularity in the scientific community, and much more.

\begin{quote}
\emph{"Python is an interpreted, object-oriented, high-level programming language with dynamic semantics. Python's simple, easy to learn syntax emphasizes readability and therefore reduces the cost of program maintenance. Python supports modules and packages, which encourages program modularity and code reuse. The Python interpreter and the extensive standard library are available in source or binary form without charge for all major platforms, and can be freely distributed."} \cite{python-definition}
\end{quote}



\section{Tools}



\section{Use cases}



\chapter{Measurements of plain bearings at available test stands.}

The main measurement will focus on trying to analyse the loss of material in the inner surface of the sleeve which works as a plain bearing. The surface of the sleeve is asymmetrically eroded and the main task in analysing this measurement will be to determine the volume of material loss. 

\section{Measurement stand}

For this purpose a measuring stand has been constructed as shown in Figure \ref{test-stand}. The stand consists of: 
\begin{itemize}
	\item control panel - used to determine the method of measurement, among others the angle of rotation of the measured sleeve, the task of displacement of the sensor for subsequent angular measurements
	\item rotary table - the measured sleeve is fixed to it, driven by a belt drive
	\item sensor - attached to the vertical axis, built on a trapezoidal screw
	\item other mechanical and electronic parts - drives, housings, control system, cable interface
\end{itemize}

\begin{figure}[htbp]
  \begin{center}
    \resizebox{0.7\textwidth}{!}{
      \includegraphics{images/test-stand.png}
    }
    \caption[Measurement stand]{Measurement stand.}
    \label{test-stand}
  \end{center}
\end{figure}

When it comes to the measuring station itself, it is worth mentioning that its inaccuracy must be taken into account. The measurement accuracy of the sensor exceeds the precision with which the position of the sensor can be set. Z-axis built on a trapezoidal screw with the driving motor has a height error of 0.1mm (?), while the rotary table has a position accuracy of 0.1 degree (?) and there is also the error of making the rotary plate of 0.05mm (?).
While analysing the measurement results, there will also be an attempt to find these bugs and neutralise them if possible.

The data that are received from the measuring station are prepared by the software attached to the sensors themselves in a specific way. The data is immediately converted into Cartesian coordinates and additionally the information about the workstation settings for a particular measurement is added. For analysis, data are retrieved from .csv files, a format supported by both spreadsheet programs and easy to load into Python.

\section{Measuring sleeve}

Figure \ref{test-sleeve} shows an example of the sleeve that will be measured, one can see that the sleeve consists of two materials - an outer metal material and an inner plastic layer. 

\begin{figure}[htbp]
  \begin{center}
    \resizebox{0.7\textwidth}{!}{
      \includegraphics{images/sleeve.png}
    }
    \caption[Example of measurement sleeve]{Example of measurement sleeve.}
    \label{test-sleeve}
  \end{center}
\end{figure}

A significant loss of inner layer material on one side of the sleeve can be observed. The object of the analysis will be to determine as precisely as possible the volume of this loss based on measurements taken at several heights of the sleeve.

\chapter{Use of data science methods to analyse obtained data from plain bearing measurements.}

\chapter{Development of the analysis results and conclusions.}

% bibliography
\bibliographystyle{plain}                      
\begin{thebibliography}{3}                      % bibliography enviroment begining
\addcontentsline{toc}{chapter}{Bibliography}    % adding bibliography to the table of content
\small              

\bibitem{Hands-On} 
A. Geron,\emph{Hands-On Machine Learning with Scikit-Learn and TensorFlow}, O'Reilly Media, Inc., 2017

\bibitem{Statistics}
P. Bruce, A. Bruce, \emph{Practical Statistics for Data Scientists}, O'Reilly Media, Inc. 2017

\bibitem{dsgraph}
https://thedatascientist.com/data-science-without-programming/

\bibitem{dsdefinition}
https://towardsdatascience.com/data-science-101-life-cycle-of-a-data-science-project-86cbc4a2f7f0

\bibitem{linkexcel}
https://www.microsoft.com/en-us/microsoft-365/blog/2019/02/06/excel-with-microsoft-excel-in-office-365/

\bibitem{linkexceldefinition}
https://www.opengatesw.net/ms-excel-tutorials/

\bibitem{linkorigin}
https://www.originlab.com/index.aspx?go=PRODUCTS/Origin

\bibitem{linklabview}
https://www.ni.com/pl-pl/shop/labview.html

\bibitem{python-definition}
https://www.python.org/doc/essays/blurb/

\end{thebibliography}                 
\end{document}

%EOF